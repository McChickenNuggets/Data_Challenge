![image](https://github.com/McChickenNuggets/Data_Challenge/blob/master/img/Team_Outlier.png)
# Data_Challenge

This is a project focusing on the impacts of Proposition 16. We retrieve data from Twitter using rtweet and twitteR, and use text mining through the combining results of 3 differnt sentiment-analysis to analyze the opposition and support implied by the contexts of each tweet on Proposition 16. Using the result generated by sentiment-analysis, we construct a logistic model using the filtered-dataset and investigate the effectiveness, sensitivity, specificity of our model. Besides, combining the past admission summaries based on ethnicity from University of Califonia, we generalize ideas from social medias to make assumptions about the following conseuences of the Proposition 16 on college admission based on ethnicity.

## Key Techniques!
  - Web Scraping
  - Data Processing
  - Text Mining
  - Logistic Regression
  - Data Visualization

##  How controversial is the Proposition 16?
Collect the number of relevant tweets for each pp (15~24)
```r
tw15= length(twitteR::searchTwitter('#Prop15', n=1e4, since = '2020-09-01', retryOnRateLimit = 1e3))
tw16= length(twitteR::searchTwitter('#Prop16', n=1e4, since = '2020-09-01', retryOnRateLimit = 1e3))
tw17= length(twitteR::searchTwitter('#Prop17', n=1e4, since = '2020-09-01', retryOnRateLimit = 1e3))
tw18= length(twitteR::searchTwitter('#Prop18', n=1e4, since = '2020-09-01', retryOnRateLimit = 1e3))
tw19= length(twitteR::searchTwitter('#Prop19', n=1e4, since = '2020-09-01', retryOnRateLimit = 1e3))
tw20= length(twitteR::searchTwitter('#Prop20', n=1e4, since = '2020-09-01', retryOnRateLimit = 1e3))
tw21= length(twitteR::searchTwitter('#Prop21', n=1e4, since = '2020-09-01', retryOnRateLimit = 1e3))
tw22= length(twitteR::searchTwitter('#Prop22', n=1e4, since = '2020-09-01', retryOnRateLimit = 1e3))
tw23= length(twitteR::searchTwitter('#Prop23', n=1e4, since = '2020-09-01', retryOnRateLimit = 1e3))    
tw24= length(twitteR::searchTwitter('#Prop24', n=1e4, since = '2020-09-01', retryOnRateLimit = 1e3))
```
Compare 16 with the other 9 pp with bar graph 
```r
ggplot(data_pie1, aes(x=Proposition, y=num, fill=Proposition)) +
  geom_bar(stat="identity")+theme_minimal()+ 
  ggtitle("Proposition Popularity Share") + 
  ylab("Number of Tweets")
```

![image](https://github.com/McChickenNuggets/Data_Challenge/blob/master/img/Proposition%20popularity%20share.png)

Based on the proposition popularity share, we can see Proposition 16 is the most controversial one among all the propositions. In the past 30 days, there are nearly 3000 tweets that contain the hashtag of #Prop16. Therefore, we decide to foucs on the ultermost controversial proposition 16 for our data challenge.

## Web Scraping
Search for up to 15,000 (non-retweeted) tweets containing the #Prop16 hashtag.
```r
# Retrieve System Time A week ago
toDate <- format(Sys.time() - 60 * 60 * 24 * 7, "%Y%m%d%H%M")

# Search information for #Prop 16 for past 30days in USA
rt <- search_30day("#Prop16", n = 15000,
                   env_name = "research", toDate = toDate,
                   geocode = lookup_coords("usa"))
```
Quickly visualize frequency of tweets over time using `ts_plot()`
```r
# Time_plot to display the frequencies of post on twitter about #Prop16
rt %>%
  ts_plot("3 hours") +
  theme_minimal() +
  theme(plot.title = element_text(face = "bold")) +
  labs(
    x = NULL, y = NULL,
    title = "Frequency of #Prop16 Twitter statuses from past 30 days",
    subtitle = "Twitter status (tweet) counts aggregated using three-hour intervals",
    caption = "\nSource: Data collected from Twitter's REST API via rtweet"
  )
```
Visualize the geographical location of these tweets
```r
# Get geographical location
geo <- lat_lng(rt)

## plot california boundaries
par(mar = c(0, 0, 0, 0))
map('state', region = "california",lwd=.25)

## plot lat and lng points onto state map
with(geo, points(lng, lat, pch = 20, cex = .75, col = rgb(0, .3, .7, .75)))
```
## Data Processing
Filter the Dataset
```r
# Filter important attributes
rt<- rt %>% select(user_id,screen_name:source,is_quote:hashtags,lang,name:verified,-url,-account_created_at)

# Check whether each attribute is convertible
sapply(rt,class)

# Transform the attribute
rt$hashtags<-as.character(rt$hashtags)

# Write to CSV file
write.csv(rt,"C:\\Users\\Jaune\\Desktop\\pp16twitter.csv")
```
Import the Dataset and Reprocess
```r
pp16twitter <- read_csv("pp16twitter.csv", col_types = cols())

# redefine original data as new df for analysis
pp16twi <- pp16twitter %>% 
  drop_na(c(hashtags, location)) %>%
  transmute(
    support = case_when(
    str_detect(hashtags, regex("(yes)|(Opportunity4All)", ignore_case=TRUE)) ~ "yes",
    str_detect(hashtags, regex("(no)|(stop)|(unitynotdivision)", ignore_case=TRUE)) ~ "no",
    TRUE ~ "undef"
  ), 
  text, 
  inCA = ifelse(str_detect(location, regex("(CA)|(California)", ignore_case=TRUE)), "yes", "no"), 
  influential = case_when(
    followers_count > 1000 ~ "yes",
    followers_count < 1000 ~ "no"
  ),
  username = screen_name,
  bio = description
  )

# twitter dataset split into 2 parts: df1 - yes/no defined in hashtags & df2 - those undefined
df1 <- pp16twi %>% filter(support != "undef")
df2 <- pp16twi %>% filter(support == "undef")

# those undefined (df2) need to be defined based on text sentiment analysis 
textdf <- df2 %>% mutate(id = 1:nrow(df2))
```
## Text Mining (Sentiment Analysis)
#### 1st method: unigram sentiment analysis for tweets with 'bing' lexicon
```{r}
# each tweet split into single word and take out stop words
tweet_token <- textdf %>% 
  unnest_tokens(word, text) %>%
  anti_join(stop_words) %>%
  group_by(id) %>%
  count(word,sort = TRUE) %>%
  arrange(id)

# freq of each word in 
